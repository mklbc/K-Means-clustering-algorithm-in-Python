# -*- coding: utf-8 -*-
"""Lab_1_Kulubecioglu_Mehmet_Statistical_Methods_Of_ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NZA7CaEQvsocHIl9R_Gd930afRGB0mL6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# clustering.csv dosyasını oku
data = pd.read_csv('clustering.csv')

# İlk 5 satırı görüntüle
print(data.head())

X = data[["LoanAmount", "ApplicantIncome"]]

# Veriyi görselleştirelim
plt.scatter(X["ApplicantIncome"], X["LoanAmount"], c='black')
plt.xlabel('Annual Income')
plt.ylabel('Loan Amount (In Thousands)')
plt.show()

K = 3  # Küme sayısı
Centroids = X.sample(n=K)  # Rastgele seçilmiş küme merkezleri

# Veri noktalarını ve küme merkezlerini çizelim
plt.scatter(X["ApplicantIncome"], X["LoanAmount"], c='black')
plt.scatter(Centroids["ApplicantIncome"], Centroids["LoanAmount"], c='red')
plt.xlabel('Annual Income')
plt.ylabel('Loan Amount (In Thousands)')
plt.show()

# K-Means modeli oluştur
kmeans = KMeans(n_clusters=K, init='k-means++', random_state=42)

# Modeli eğit ve tahmin yap
clusters = kmeans.fit_predict(X)

# Küme merkezlerini al
centroids = kmeans.cluster_centers_

# Sonuçları çizelim
plt.scatter(X["ApplicantIncome"], X["LoanAmount"], c=clusters, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)
plt.xlabel('Annual Income')
plt.ylabel('Loan Amount (In Thousands)')
plt.show()

SSE = []  # Toplam hata kareleri listesi
for cluster in range(1, 10):  # 1'den 10'a kadar kümeler dene
    kmeans = KMeans(n_clusters=cluster, init='k-means++', random_state=42)
    kmeans.fit(X)
    SSE.append(kmeans.inertia_)  # Her küme için hata kareleri toplamı

# Grafiği çizelim
plt.plot(range(1, 10), SSE, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia (SSE)')
plt.title('Elbow Method for Optimal Clusters')
plt.show()

data = pd.read_csv('Wholesale customers data.csv')

# İlk 5 satırı görüntüle
print(data.head())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# Standartlaştırılmış verinin istatistiklerine bakalım
pd.DataFrame(data_scaled).describe()

kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
kmeans.fit(data_scaled)

# Küme tahminlerini al
predictions = kmeans.predict(data_scaled)

# Küme sonuçlarını ekleyelim
data['Cluster'] = predictions

# Küme sayısına göre veri dağılımı
print(data['Cluster'].value_counts())

plt.figure(figsize=(10, 6))
plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=predictions, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Wholesale Customers Clustering')
plt.show()